#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Step 2: í›„ì²˜ë¦¬ + í‰ê°€ (Score ë¡œë“œ, ì¬í•™ìŠµ ì—†ìŒ)

Usage:
    python scripts/run_step2.py --dataset PSM
    python scripts/run_step2.py --dataset PSM --postprocess T1 T2 T3
    python scripts/run_step2.py --dataset PSM --models DLinear
    python scripts/run_step2.py --dataset PSM --log_dir logs/

í›„ì²˜ë¦¬ ì „ëµ:
    T1: Fixed Threshold (3Ïƒ)
    T2: Fixed Threshold (99th Percentile)
    T3: EWMA Adaptive Threshold
    T4: Score Smoothing + Fixed (3Ïƒ)
    T5: Score Smoothing + EWMA Adaptive
"""

import argparse
import sys
import os
import logging
from datetime import datetime

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)
sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))


def setup_logging(log_dir: str, dataset: str, postprocess: list):
    """ë¡œê¹… ì„¤ì •"""
    os.makedirs(log_dir, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    postprocess_str = '-'.join(postprocess)
    log_filename = f"{timestamp}_{dataset}_{postprocess_str}_step2.log"
    log_path = os.path.join(log_dir, log_filename)

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )

    return log_path


def log(msg):
    """ë¡œê¹… ë˜ëŠ” print"""
    if logging.getLogger().handlers:
        logging.info(msg)
    else:
        print(msg)

import numpy as np
import pandas as pd

from data import DataLoader
from preprocessing import EWMASmoother
from postprocessing import FixedThreshold, EWMAThreshold
from evaluation import PointF1, PointAdjustF1, AUCMetrics
from utils import ExperimentTracker


# ============================================================
# í›„ì²˜ë¦¬ ì „ëµ ì •ì˜
# ============================================================

def clip_extreme_scores(scores, lower_percentile=0.1, upper_percentile=99.9):
    """
    ê·¹ë‹¨ê°’ í´ë¦¬í•‘ (ìˆ˜ì¹˜ ì•ˆì •ì„±)

    Args:
        scores: Anomaly Score ë°°ì—´
        lower_percentile: í•˜ìœ„ ë°±ë¶„ìœ„
        upper_percentile: ìƒìœ„ ë°±ë¶„ìœ„

    Returns:
        í´ë¦¬í•‘ëœ scores
    """
    lower = np.percentile(scores, lower_percentile)
    upper = np.percentile(scores, upper_percentile)
    return np.clip(scores, lower, upper)


def apply_postprocess(scores, postprocess_id, train_scores=None):
    """
    í›„ì²˜ë¦¬ ì „ëµ ì ìš©

    Args:
        scores: Anomaly Score ë°°ì—´
        postprocess_id: í›„ì²˜ë¦¬ ID ('T1' ~ 'T5')
        train_scores: í•™ìŠµ ë°ì´í„° Score (ì„ê³„ê°’ í•™ìŠµìš©, optional)

    Returns:
        predictions: ì´ì§„ ì˜ˆì¸¡ (0/1)
        threshold_value: ì‚¬ìš©ëœ ì„ê³„ê°’
    """
    # ê·¹ë‹¨ê°’ í´ë¦¬í•‘ (ìˆ˜ì¹˜ ì•ˆì •ì„±)
    scores = clip_extreme_scores(scores)

    fit_scores = train_scores if train_scores is not None else scores
    if train_scores is not None:
        fit_scores = clip_extreme_scores(fit_scores)

    if postprocess_id == 'T1':
        # Fixed Threshold (3Ïƒ)
        threshold = FixedThreshold(method='sigma', n_sigma=3.0)
        threshold.fit(fit_scores)
        predictions = threshold.apply(scores)
        return predictions, threshold.threshold_

    elif postprocess_id == 'T2':
        # Fixed Threshold (99th Percentile)
        threshold = FixedThreshold(method='percentile', percentile=99.0)
        threshold.fit(fit_scores)
        predictions = threshold.apply(scores)
        return predictions, threshold.threshold_

    elif postprocess_id == 'T3':
        # EWMA Adaptive Threshold
        threshold = EWMAThreshold(span=100, n_sigma=3.0)
        threshold.fit(fit_scores)
        predictions = threshold.apply(scores)
        # EWMAëŠ” ë™ì  ì„ê³„ê°’ì´ë¯€ë¡œ í‰ê· ê°’ ë°˜í™˜
        return predictions, np.mean(threshold.threshold_)

    elif postprocess_id == 'T4':
        # Score Smoothing + Fixed (3Ïƒ)
        smoother = EWMASmoother(span=5)
        smoother.fit(scores.reshape(-1, 1))  # fit ì¶”ê°€
        smoothed_scores = smoother.transform(scores.reshape(-1, 1)).flatten()

        threshold = FixedThreshold(method='sigma', n_sigma=3.0)
        if train_scores is not None:
            smoothed_train = smoother.transform(train_scores.reshape(-1, 1)).flatten()
            threshold.fit(smoothed_train)
        else:
            threshold.fit(smoothed_scores)

        predictions = threshold.apply(smoothed_scores)
        return predictions, threshold.threshold_

    elif postprocess_id == 'T5':
        # Score Smoothing + EWMA Adaptive
        smoother = EWMASmoother(span=5)
        smoother.fit(scores.reshape(-1, 1))  # fit ì¶”ê°€
        smoothed_scores = smoother.transform(scores.reshape(-1, 1)).flatten()

        threshold = EWMAThreshold(span=100, n_sigma=3.0)
        if train_scores is not None:
            smoothed_train = smoother.transform(train_scores.reshape(-1, 1)).flatten()
            threshold.fit(smoothed_train)
        else:
            threshold.fit(smoothed_scores)

        predictions = threshold.apply(smoothed_scores)
        return predictions, np.mean(threshold.threshold_)

    else:
        raise ValueError(f"Unknown postprocess_id: {postprocess_id}")


# ============================================================
# í‰ê°€ í•¨ìˆ˜
# ============================================================

def evaluate(predictions, labels, scores=None):
    """
    í‰ê°€ ì§€í‘œ ê³„ì‚°

    Args:
        predictions: ì´ì§„ ì˜ˆì¸¡ (0/1)
        labels: Ground Truth (0/1)
        scores: ì›ë³¸ ì´ìƒì¹˜ ì ìˆ˜ (AUC ê³„ì‚°ìš©, optional)

    Returns:
        metrics: í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    """
    # Point F1
    point_metric = PointF1()
    point_result = point_metric.compute(labels, predictions)

    # PA F1
    pa_metric = PointAdjustF1()
    pa_result = pa_metric.compute(labels, predictions)

    metrics = {
        'point_precision': point_result['precision'],
        'point_recall': point_result['recall'],
        'point_f1': point_result['f1'],
        'pa_precision': pa_result['precision'],
        'pa_recall': pa_result['recall'],
        'pa_f1': pa_result['f1']
    }

    # AUC ì§€í‘œ (scoresê°€ ìˆì„ ë•Œë§Œ)
    if scores is not None:
        auc_metric = AUCMetrics()
        auc_result = auc_metric.compute(labels, scores)
        metrics['roc_auc'] = auc_result['roc_auc']
        metrics['pr_auc'] = auc_result['pr_auc']

    return metrics


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

def main():
    parser = argparse.ArgumentParser(description='Step 2: í›„ì²˜ë¦¬ + í‰ê°€')
    parser.add_argument('--dataset', type=str, default='PSM',
                        choices=['PSM', 'SWaT'], help='ë°ì´í„°ì…‹')
    parser.add_argument('--models', nargs='+', default=None,
                        help='í‰ê°€í•  ëª¨ë¸ (ë¯¸ì§€ì •ì‹œ ì „ì²´)')
    parser.add_argument('--preprocess', nargs='+', default=None,
                        help='í‰ê°€í•  ì „ì²˜ë¦¬ (ë¯¸ì§€ì •ì‹œ ì „ì²´)')
    parser.add_argument('--postprocess', nargs='+',
                        default=['T1', 'T2', 'T3', 'T4', 'T5'],
                        help='í›„ì²˜ë¦¬ ì „ëµ ëª©ë¡')
    parser.add_argument('--log_dir', type=str, default=None, help='ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬')

    args = parser.parse_args()

    # ë¡œê¹… ì„¤ì •
    if args.log_dir:
        log_path = setup_logging(args.log_dir, args.dataset, args.postprocess)
        logging.info(f"Log file: {log_path}")

    log("=" * 70)
    log("Step 2: í›„ì²˜ë¦¬ + í‰ê°€ (Score ë¡œë“œ, ì¬í•™ìŠµ ì—†ìŒ)")
    log("=" * 70)
    log(f"Dataset: {args.dataset}")
    log(f"Postprocess: {args.postprocess}")
    log("=" * 70)

    # Ground Truth ë¡œë“œ
    data_path = os.path.join(PROJECT_ROOT, 'data', 'raw', args.dataset)
    loader = DataLoader(args.dataset, data_path)
    _, test_labels = loader.load_test()

    log(f"Test labels shape: {test_labels.shape}")
    log(f"Anomaly ratio: {test_labels.mean()*100:.2f}%")

    # ExperimentTracker ì´ˆê¸°í™”
    tracker = ExperimentTracker(base_dir=os.path.join(PROJECT_ROOT, 'outputs'))

    # ì‚¬ìš© ê°€ëŠ¥í•œ Score ëª©ë¡
    available_scores = tracker.list_available_scores()

    # í•„í„°ë§
    if args.models:
        available_scores = [s for s in available_scores if s['model'] in args.models]
    if args.preprocess:
        available_scores = [s for s in available_scores if s['preprocess'] in args.preprocess]

    # í•´ë‹¹ ë°ì´í„°ì…‹ë§Œ
    available_scores = [s for s in available_scores if s['dataset'] == args.dataset]

    if not available_scores:
        log("\nâš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ Scoreê°€ ì—†ìŠµë‹ˆë‹¤.")
        log("ë¨¼ì € run_step1.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return

    log(f"\ní‰ê°€í•  Score: {len(available_scores)}ê°œ")
    for s in available_scores:
        log(f"  - {s['model']}_{s['preprocess']}_{s['dataset']}")

    # ì‹¤í—˜ ì‹¤í–‰
    total_exp = len(available_scores) * len(args.postprocess)
    current_exp = 0

    results = []

    for score_info in available_scores:
        model = score_info['model']
        preprocess = score_info['preprocess']

        log(f"\n{'â”€' * 50}")
        log(f"Score: {model} + {preprocess}")
        log(f"{'â”€' * 50}")

        # Score ë¡œë“œ
        scores = tracker.load_scores(model, preprocess, args.dataset)
        log(f"  Score ë¡œë“œ ì™„ë£Œ: shape={scores.shape}")

        # ê¸¸ì´ ë§ì¶”ê¸°
        min_len = min(len(scores), len(test_labels))
        scores = scores[:min_len]
        labels = test_labels[:min_len]

        for postprocess_id in args.postprocess:
            current_exp += 1
            log(f"\n  [{current_exp}/{total_exp}] {postprocess_id}")

            try:
                # í›„ì²˜ë¦¬ ì ìš©
                predictions, threshold_value = apply_postprocess(scores, postprocess_id)

                # í‰ê°€
                metrics = evaluate(predictions, labels, scores=scores)

                # ê¸°ë¡
                tracker.log_evaluation(
                    model=model,
                    preprocess=preprocess,
                    postprocess=postprocess_id,
                    dataset=args.dataset,
                    metrics=metrics,
                    threshold_value=threshold_value
                )

                # ê²°ê³¼ ì €ì¥
                results.append({
                    'model': model,
                    'preprocess': preprocess,
                    'postprocess': postprocess_id,
                    **metrics
                })

                log(f"    Point F1: {metrics['point_f1']:.4f}")
                log(f"    PA F1: {metrics['pa_f1']:.4f}")
                log(f"    ROC-AUC: {metrics.get('roc_auc', 'N/A'):.4f}" if 'roc_auc' in metrics else "")
                log(f"    PR-AUC: {metrics.get('pr_auc', 'N/A'):.4f}" if 'pr_auc' in metrics else "")

            except Exception as e:
                log(f"    âŒ Error: {e}")
                import traceback
                traceback.print_exc()

    # ê²°ê³¼ ìš”ì•½
    log("\n" + "=" * 70)
    log("Step 2 ì™„ë£Œ!")
    log("=" * 70)

    if results:
        df = pd.DataFrame(results)

        # PA F1 ê¸°ì¤€ ì •ë ¬
        df_sorted = df.sort_values('pa_f1', ascending=False)

        log("\nğŸ“Š PA F1 Top 5:")
        log("-" * 70)
        top5 = df_sorted.head(5)[['model', 'preprocess', 'postprocess', 'pa_f1', 'point_f1']]
        log(top5.to_string(index=False))

        # í”¼ë²— í…Œì´ë¸”
        log("\nğŸ“Š í›„ì²˜ë¦¬ë³„ PA F1 (í‰ê· ):")
        log("-" * 70)
        pivot = df.pivot_table(
            index='postprocess',
            columns='model',
            values='pa_f1',
            aggfunc='mean'
        ).round(4)
        log(pivot.to_string())

    tracker.print_summary()


if __name__ == '__main__':
    main()
