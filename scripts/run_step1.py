#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Step 1: ì „ì²˜ë¦¬ + ëª¨ë¸ í•™ìŠµ + Score ì €ì¥

Usage:
    python scripts/run_step1.py --dataset PSM
    python scripts/run_step1.py --dataset PSM --models DLinear
    python scripts/run_step1.py --dataset PSM --preprocess P_MM P_STD
    python scripts/run_step1.py --dataset PSM --gpu 0 --log_dir logs/
"""

import argparse
import sys
import os
import time
import logging
from datetime import datetime

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)
sys.path.insert(0, os.path.join(PROJECT_ROOT, 'src'))


def setup_logging(log_dir: str, dataset: str, models: list, preprocess: list):
    """ë¡œê¹… ì„¤ì •"""
    os.makedirs(log_dir, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    models_str = '-'.join(models)
    preprocess_str = '-'.join(preprocess)
    log_filename = f"{timestamp}_{dataset}_{models_str}_{preprocess_str}_step1.log"
    log_path = os.path.join(log_dir, log_filename)

    # ë¡œê±° ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )

    return log_path


def log(msg):
    """ë¡œê¹… ë˜ëŠ” print"""
    if logging.getLogger().handlers:
        logging.info(msg)
    else:
        print(msg)


import numpy as np

from data import DataLoader
from preprocessing import Scaler, EWMASmoother, MovingAverageDetrender
from models import DLinearModel, OmniAnomaly
from utils import ExperimentTracker

# ì„¤ì • íŒŒì¼
from config import get_model_config, DLINEAR_CONFIG, OMNIANOMALY_CONFIG


# ============================================================
# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# ============================================================

def apply_preprocessing(train_data, test_data, preprocess_id):
    """
    ì „ì²˜ë¦¬ ì ìš©

    Args:
        train_data: í•™ìŠµ ë°ì´í„° (numpy array)
        test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„° (numpy array)
        preprocess_id: ì „ì²˜ë¦¬ ID ('P_MM', 'P_STD', 'P_SM', 'P_DT')

    Returns:
        train_processed, test_processed
    """

    if preprocess_id == 'P_MM':
        # MinMax ì •ê·œí™”ë§Œ
        scaler = Scaler(method='minmax')
        scaler.fit(train_data)
        return scaler.transform(train_data), scaler.transform(test_data)

    elif preprocess_id == 'P_STD':
        # Standard ì •ê·œí™”ë§Œ
        scaler = Scaler(method='standard')
        scaler.fit(train_data)
        return scaler.transform(train_data), scaler.transform(test_data)

    elif preprocess_id == 'P_SM':
        # MinMax + EWMA Smoothing
        scaler = Scaler(method='minmax')
        scaler.fit(train_data)
        train_scaled = scaler.transform(train_data)
        test_scaled = scaler.transform(test_data)

        smoother = EWMASmoother(span=5)
        smoother.fit(train_scaled)  # fit ì¶”ê°€
        return smoother.transform(train_scaled), smoother.transform(test_scaled)

    elif preprocess_id == 'P_DT':
        # MinMax + Detrending
        scaler = Scaler(method='minmax')
        scaler.fit(train_data)
        train_scaled = scaler.transform(train_data)
        test_scaled = scaler.transform(test_data)

        detrender = MovingAverageDetrender(window=10)
        detrender.fit(train_scaled)
        return detrender.transform(train_scaled), detrender.transform(test_scaled)

    else:
        raise ValueError(f"Unknown preprocess_id: {preprocess_id}")


# ============================================================
# ëª¨ë¸ í•™ìŠµ
# ============================================================

def train_and_score(model_name, train_data, test_data, config=None):
    """
    ëª¨ë¸ í•™ìŠµ ë° Anomaly Score ê³„ì‚°

    Args:
        model_name: 'DLinear' ë˜ëŠ” 'OmniAnomaly'
        train_data: ì „ì²˜ë¦¬ëœ í•™ìŠµ ë°ì´í„°
        test_data: ì „ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        config: ëª¨ë¸ ì„¤ì • (optional)

    Returns:
        scores: Anomaly Score (í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸¸ì´)
        training_time: í•™ìŠµ ì†Œìš” ì‹œê°„ (ì´ˆ)
    """

    if model_name == 'DLinear':
        model = DLinearModel(
            seq_len=config.get('seq_len', 100) if config else 100,
            pred_len=config.get('pred_len', 1) if config else 1,
            epochs=config.get('epochs', 50) if config else 50,
            batch_size=config.get('batch_size', 32) if config else 32,
            lr=config.get('lr', 0.005) if config else 0.005,
            early_stopping=config.get('early_stopping', True) if config else True,
            patience=config.get('patience', 5) if config else 5,
            val_ratio=config.get('val_ratio', 0.1) if config else 0.1,
        )

    elif model_name == 'OmniAnomaly':
        # OmniAnomalyëŠ” config dictë¥¼ ë°›ìŒ (ë…¼ë¬¸ ì›ë³¸ ì„¤ì •)
        omni_config = {
            'window_length': config.get('window_size', 100) if config else 100,
            'hidden_dim': config.get('hidden_size', 500) if config else 500,
            'z_dim': config.get('z_dim', 3) if config else 3,
            'n_flows': config.get('n_flows', 20) if config else 20,  # ë…¼ë¬¸: 20
            'epochs': config.get('epochs', 20) if config else 20,
            'batch_size': config.get('batch_size', 50) if config else 50,
            'learning_rate': config.get('lr', 0.001) if config else 0.001,
            'weight_decay': config.get('weight_decay', 1e-4) if config else 1e-4,  # L2 ì •ê·œí™”
            'early_stopping': config.get('early_stopping', True) if config else True,
            'patience': config.get('patience', 5) if config else 5,
            'val_ratio': config.get('val_ratio', 0.3) if config else 0.3,  # ë…¼ë¬¸: 30%
        }
        model = OmniAnomaly(omni_config)

    else:
        raise ValueError(f"Unknown model: {model_name}")

    # í•™ìŠµ
    start_time = time.time()
    model.fit(train_data, verbose=True)
    training_time = time.time() - start_time

    # Score ê³„ì‚°
    scores = model.get_anomaly_score(test_data)

    return scores, training_time


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================

def main():
    parser = argparse.ArgumentParser(description='Step 1: Training + Score ì €ì¥')
    parser.add_argument('--dataset', type=str, default='PSM',
                        choices=['PSM', 'SWaT'], help='ë°ì´í„°ì…‹')
    parser.add_argument('--models', nargs='+', default=['DLinear', 'OmniAnomaly'],
                        help='í•™ìŠµí•  ëª¨ë¸ ëª©ë¡')
    parser.add_argument('--preprocess', nargs='+',
                        default=['P_MM', 'P_STD', 'P_SM', 'P_DT'],
                        help='ì „ì²˜ë¦¬ ID ëª©ë¡')
    parser.add_argument('--epochs', type=int, default=None, help='í•™ìŠµ ì—í­ (ë¯¸ì§€ì •ì‹œ ë ˆí¼ëŸ°ìŠ¤ ê¸°ë³¸ê°’)')
    parser.add_argument('--batch_size', type=int, default=None, help='ë°°ì¹˜ í¬ê¸° (ë¯¸ì§€ì •ì‹œ ë ˆí¼ëŸ°ìŠ¤ ê¸°ë³¸ê°’)')
    parser.add_argument('--seq_len', type=int, default=None, help='ì‹œí€€ìŠ¤ ê¸¸ì´ (ë¯¸ì§€ì •ì‹œ ë ˆí¼ëŸ°ìŠ¤ ê¸°ë³¸ê°’)')
    parser.add_argument('--gpu', type=int, default=None, help='ì‚¬ìš©í•  GPU ID (ë¯¸ì§€ì •ì‹œ ìë™)')
    parser.add_argument('--log_dir', type=str, default=None, help='ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬')

    args = parser.parse_args()

    # GPU ì„¤ì •
    if args.gpu is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)

    # ë¡œê¹… ì„¤ì •
    if args.log_dir:
        log_path = setup_logging(args.log_dir, args.dataset, args.models, args.preprocess)
        logging.info(f"Log file: {log_path}")

    # ============================================================
    # ë ˆí¼ëŸ°ìŠ¤ ê¸°ë°˜ ê¸°ë³¸ ì„¤ì •
    # ============================================================
    # OmniAnomaly ì›ë³¸: window=100, batch=50, lr=0.001, epochs=10
    # DLinear ì›ë³¸: seq_len=336, batch=32, lr=0.005, epochs=10
    #
    # ë°ì´í„°ì…‹ íŠ¹ì„± ê³ ë ¤:
    # - PSM: 132K samples, 25 features â†’ ì¤‘ì†Œê·œëª¨
    # - SWaT: 395K samples, 51 features â†’ ëŒ€ê·œëª¨
    # ============================================================

    REFERENCE_CONFIGS = {
        'DLinear': {
            'seq_len': 100,      # ì´ìƒíƒì§€ìš©ìœ¼ë¡œ 336â†’100 ì¡°ì • (ì›ë³¸ì€ ì¥ê¸°ì˜ˆì¸¡ìš©)
            'pred_len': 1,       # 1-step ahead ì˜ˆì¸¡
            'epochs': 10,        # ë ˆí¼ëŸ°ìŠ¤ ê¸°ë³¸ê°’
            'batch_size': 32,    # ë ˆí¼ëŸ°ìŠ¤ ê¸°ë³¸ê°’
            'lr': 0.005          # ë ˆí¼ëŸ°ìŠ¤ ê¸°ë³¸ê°’
        },
        'OmniAnomaly': {
            # ë…¼ë¬¸ ì›ë³¸ ì„¤ì • (KDD 2019)
            'window_size': 100,   # T+1 = 100
            'hidden_size': 500,   # GRU units
            'z_dim': 3,           # Latent dimension
            'n_flows': 20,        # Planar Normalizing Flow layers
            'epochs': 20,         # with early stopping
            'batch_size': 50,     # ë…¼ë¬¸ ì›ë³¸
            'lr': 0.001,          # Initial learning rate
            'val_ratio': 0.3,     # 30% validation split
            'weight_decay': 1e-4  # L2 regularization
        }
    }

    log("=" * 70)
    log("Step 1: ì „ì²˜ë¦¬ + ëª¨ë¸ í•™ìŠµ + Score ì €ì¥")
    log("=" * 70)
    log(f"Dataset: {args.dataset}")
    log(f"Models: {args.models}")
    log(f"Preprocess: {args.preprocess}")
    if args.gpu is not None:
        log(f"GPU: {args.gpu}")
    log("=" * 70)

    # ë°ì´í„° ë¡œë“œ
    data_path = os.path.join(PROJECT_ROOT, 'data', 'raw', args.dataset)
    loader = DataLoader(args.dataset, data_path)
    train_data = loader.load_train()
    test_data, test_labels = loader.load_test()

    train_np = train_data.values
    test_np = test_data.values

    log(f"Train shape: {train_np.shape}")
    log(f"Test shape: {test_np.shape}")
    log(f"Anomaly ratio: {test_labels.mean()*100:.2f}%")

    # ExperimentTracker ì´ˆê¸°í™”
    tracker = ExperimentTracker(base_dir=os.path.join(PROJECT_ROOT, 'outputs'))

    # ëª¨ë¸ ì„¤ì • (CLI ì¸ìë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥)
    model_configs = {}

    for model_name in ['DLinear', 'OmniAnomaly']:
        config = REFERENCE_CONFIGS[model_name].copy()

        # CLI ì˜¤ë²„ë¼ì´ë“œ
        if args.epochs is not None:
            config['epochs'] = args.epochs
        if args.batch_size is not None:
            config['batch_size'] = args.batch_size
        if args.seq_len is not None:
            if model_name == 'DLinear':
                config['seq_len'] = args.seq_len
            else:
                config['window_size'] = args.seq_len

        model_configs[model_name] = config

    # ì„¤ì • ì¶œë ¥
    log("ğŸ“‹ Model Configurations (Reference-based):")
    for name, cfg in model_configs.items():
        log(f"  {name}: {cfg}")

    # ì‹¤í—˜ ì‹¤í–‰
    total_exp = len(args.preprocess) * len(args.models)
    current_exp = 0

    for preprocess_id in args.preprocess:
        log(f"{'â”€' * 50}")
        log(f"ì „ì²˜ë¦¬: {preprocess_id}")
        log(f"{'â”€' * 50}")

        train_processed, test_processed = apply_preprocessing(
            train_np, test_np, preprocess_id
        )
        log(f"  ì „ì²˜ë¦¬ ì™„ë£Œ: {train_processed.shape}")

        for model_name in args.models:
            current_exp += 1
            log(f"[{current_exp}/{total_exp}] {model_name} + {preprocess_id}")

            try:
                config = model_configs.get(model_name, {})
                scores, training_time = train_and_score(
                    model_name,
                    train_processed,
                    test_processed,
                    config
                )

                # ê¸°ë¡
                tracker.log_training(
                    model=model_name,
                    preprocess=preprocess_id,
                    dataset=args.dataset,
                    scores=scores,
                    training_time=training_time,
                    config=config
                )

            except Exception as e:
                log(f"  âŒ Error: {e}")
                import traceback
                traceback.print_exc()

    # ê²°ê³¼ ìš”ì•½
    log("=" * 70)
    log("Step 1 ì™„ë£Œ!")
    log("=" * 70)
    tracker.print_summary()

    # ì €ì¥ëœ Score í™•ì¸
    available = tracker.list_available_scores()
    log(f"ì €ì¥ëœ Score: {len(available)}ê°œ")
    for s in available:
        log(f"  - {s['model']}_{s['preprocess']}_{s['dataset']}")


if __name__ == '__main__':
    main()
