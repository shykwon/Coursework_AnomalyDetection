# -*- coding: utf-8 -*-
"""
ì‹¤í—˜ ê²°ê³¼ ìë™ ê¸°ë¡ ë° ê´€ë¦¬ ì‹œìŠ¤í…œ

ğŸ“ [í•™ìŠµ í¬ì¸íŠ¸]
- Step 1 (ì „ì²˜ë¦¬ + í•™ìŠµ): Score ì €ì¥ + í•™ìŠµ ì •ë³´ ê¸°ë¡
- Step 2 (í›„ì²˜ë¦¬): Score ë¡œë“œ â†’ í‰ê°€ â†’ ê²°ê³¼ ê¸°ë¡
- ë³´ê³ ì„œ ì‘ì„± ì‹œ í”¼ë²— í…Œì´ë¸”, ë¹„êµí‘œ ìë™ ìƒì„±
"""

import os
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

import numpy as np
import pandas as pd


class ExperimentTracker:
    """
    ì‹¤í—˜ ê²°ê³¼ ìë™ ê¸°ë¡ ë° ê´€ë¦¬ í´ë˜ìŠ¤

    ì‚¬ìš©ë²•:
    ```python
    tracker = ExperimentTracker(base_dir='outputs/')

    # Step 1: í•™ìŠµ í›„ Score ì €ì¥
    tracker.log_training(
        model='DLinear',
        preprocess='P_MM',
        dataset='PSM',
        scores=anomaly_scores,
        training_time=120.5
    )

    # Step 2: í›„ì²˜ë¦¬ ê²°ê³¼ ê¸°ë¡
    tracker.log_evaluation(
        model='DLinear',
        preprocess='P_MM',
        postprocess='T1',
        dataset='PSM',
        metrics={'point_f1': 0.72, 'pa_f1': 0.85, ...}
    )

    # ë³´ê³ ì„œìš© í…Œì´ë¸” ìƒì„±
    tables = tracker.generate_report()
    ```
    """

    def __init__(self, base_dir: str = 'outputs/'):
        """
        Args:
            base_dir: ê²°ê³¼ ì €ì¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬
        """
        self.base_dir = Path(base_dir)
        self.scores_dir = self.base_dir / 'scores'
        self.results_dir = self.base_dir / 'results'
        self.figures_dir = self.base_dir / 'figures'
        self.logs_dir = self.base_dir / 'logs'

        # ë””ë ‰í† ë¦¬ ìƒì„±
        for dir_path in [self.scores_dir, self.results_dir, self.figures_dir, self.logs_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

        # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        self.training_log_path = self.results_dir / 'training_log.csv'
        self.evaluation_log_path = self.results_dir / 'evaluation_results.csv'

        # ë©”ëª¨ë¦¬ ë‚´ ê²°ê³¼ ì €ì¥
        self._training_records: List[Dict] = []
        self._evaluation_records: List[Dict] = []

        # ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ
        self._load_existing_results()

    def _load_existing_results(self):
        """ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        if self.training_log_path.exists():
            df = pd.read_csv(self.training_log_path)
            self._training_records = df.to_dict('records')

        if self.evaluation_log_path.exists():
            df = pd.read_csv(self.evaluation_log_path)
            self._evaluation_records = df.to_dict('records')

    # ==================== Step 1: í•™ìŠµ ê¸°ë¡ ====================

    def log_training(
        self,
        model: str,
        preprocess: str,
        dataset: str,
        scores: np.ndarray,
        training_time: float,
        config: Optional[Dict] = None,
        extra_info: Optional[Dict] = None
    ) -> str:
        """
        Step 1: ëª¨ë¸ í•™ìŠµ ê²°ê³¼ ê¸°ë¡ + Score ì €ì¥

        Args:
            model: ëª¨ë¸ ì´ë¦„ ('DLinear', 'OmniAnomaly')
            preprocess: ì „ì²˜ë¦¬ ID ('P_MM', 'P_STD', 'P_SM', 'P_DT')
            dataset: ë°ì´í„°ì…‹ ('PSM', 'SWaT')
            scores: Anomaly Score ë°°ì—´
            training_time: í•™ìŠµ ì‹œê°„ (ì´ˆ)
            config: ëª¨ë¸/ì „ì²˜ë¦¬ ì„¤ì • (optional)
            extra_info: ì¶”ê°€ ì •ë³´ (optional)

        Returns:
            ì €ì¥ëœ Score íŒŒì¼ ê²½ë¡œ
        """
        # Score íŒŒì¼ëª… ìƒì„±
        score_filename = f"{model}_{preprocess}_{dataset}.npy"
        score_path = self.scores_dir / score_filename

        # Score ì €ì¥
        np.save(score_path, scores)

        # í•™ìŠµ ê¸°ë¡ ìƒì„±
        record = {
            'exp_id': len(self._training_records) + 1,
            'model': model,
            'preprocess': preprocess,
            'dataset': dataset,
            'score_file': score_filename,
            'score_min': float(scores.min()),
            'score_max': float(scores.max()),
            'score_mean': float(scores.mean()),
            'score_std': float(scores.std()),
            'training_time_sec': training_time,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # ì¶”ê°€ ì •ë³´ ë³‘í•©
        if config:
            record['config'] = json.dumps(config)
        if extra_info:
            record.update(extra_info)

        # ê¸°ë¡ ì¶”ê°€ ë° ì €ì¥
        self._training_records.append(record)
        self._save_training_log()

        print(f"âœ… [Training] {model} + {preprocess} + {dataset}")
        print(f"   Score saved: {score_path}")
        print(f"   Training time: {training_time:.1f}s")

        return str(score_path)

    def _save_training_log(self):
        """í•™ìŠµ ë¡œê·¸ CSV ì €ì¥"""
        df = pd.DataFrame(self._training_records)
        df.to_csv(self.training_log_path, index=False)

    # ==================== Step 2: í‰ê°€ ê¸°ë¡ ====================

    def log_evaluation(
        self,
        model: str,
        preprocess: str,
        postprocess: str,
        dataset: str,
        metrics: Dict[str, float],
        threshold_value: Optional[float] = None,
        extra_info: Optional[Dict] = None
    ) -> None:
        """
        Step 2: í›„ì²˜ë¦¬ + í‰ê°€ ê²°ê³¼ ê¸°ë¡

        Args:
            model: ëª¨ë¸ ì´ë¦„
            preprocess: ì „ì²˜ë¦¬ ID
            postprocess: í›„ì²˜ë¦¬ ID ('T1', 'T2', 'T3', 'T4', 'T5')
            dataset: ë°ì´í„°ì…‹
            metrics: í‰ê°€ ì§€í‘œ {'point_f1': ..., 'pa_f1': ..., 'roc_auc': ..., 'pr_auc': ...}
            threshold_value: ì‚¬ìš©ëœ ì„ê³„ê°’ (optional)
            extra_info: ì¶”ê°€ ì •ë³´ (optional)
        """
        # í‰ê°€ ê¸°ë¡ ìƒì„±
        record = {
            'exp_id': len(self._evaluation_records) + 1,
            'model': model,
            'preprocess': preprocess,
            'postprocess': postprocess,
            'dataset': dataset,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }

        # ë©”íŠ¸ë¦­ ì¶”ê°€
        for key, value in metrics.items():
            record[key] = value

        if threshold_value is not None:
            record['threshold'] = threshold_value

        if extra_info:
            record.update(extra_info)

        # ê¸°ë¡ ì¶”ê°€ ë° ì €ì¥
        self._evaluation_records.append(record)
        self._save_evaluation_log()

        # ì£¼ìš” ì§€í‘œ ì¶œë ¥
        pa_f1 = metrics.get('pa_f1', metrics.get('f1', 'N/A'))
        print(f"âœ… [Eval] {model} + {preprocess} + {postprocess}: PA F1 = {pa_f1}")

    def _save_evaluation_log(self):
        """í‰ê°€ ë¡œê·¸ CSV ì €ì¥"""
        df = pd.DataFrame(self._evaluation_records)
        df.to_csv(self.evaluation_log_path, index=False)

    # ==================== Score ê´€ë¦¬ ====================

    def load_scores(self, model: str, preprocess: str, dataset: str) -> np.ndarray:
        """
        ì €ì¥ëœ Score ë¡œë“œ

        Args:
            model: ëª¨ë¸ ì´ë¦„
            preprocess: ì „ì²˜ë¦¬ ID
            dataset: ë°ì´í„°ì…‹

        Returns:
            Anomaly Score ë°°ì—´
        """
        score_filename = f"{model}_{preprocess}_{dataset}.npy"
        score_path = self.scores_dir / score_filename

        if not score_path.exists():
            raise FileNotFoundError(f"Score file not found: {score_path}")

        return np.load(score_path)

    def list_available_scores(self) -> List[Dict[str, str]]:
        """ì €ì¥ëœ Score íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        scores = []
        for score_file in self.scores_dir.glob('*.npy'):
            # íŒŒì¼ëª… í˜•ì‹: {model}_{preprocess}_{dataset}.npy
            # preprocessê°€ P_MM ì²˜ëŸ¼ _ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ rsplit ì‚¬ìš©
            stem = score_file.stem
            parts = stem.rsplit('_', 1)  # ë’¤ì—ì„œë¶€í„° 1ë²ˆë§Œ split â†’ dataset ë¶„ë¦¬
            if len(parts) == 2:
                model_preprocess, dataset = parts
                # modelê³¼ preprocess ë¶„ë¦¬ (ì²« ë²ˆì§¸ _ì—ì„œ)
                mp_parts = model_preprocess.split('_', 1)
                if len(mp_parts) == 2:
                    model, preprocess = mp_parts
                    scores.append({
                        'model': model,
                        'preprocess': preprocess,
                        'dataset': dataset,
                        'file': str(score_file)
                    })
        return scores

    # ==================== ë³´ê³ ì„œ ìƒì„± ====================

    def generate_report(self) -> Dict[str, pd.DataFrame]:
        """
        ë³´ê³ ì„œìš© í…Œì´ë¸” ìƒì„±

        Returns:
            dict with:
                - 'full': ì „ì²´ ê²°ê³¼ DataFrame
                - 'preprocess_comparison': ì „ì²˜ë¦¬ë³„ ë¹„êµ (í”¼ë²—)
                - 'postprocess_comparison': í›„ì²˜ë¦¬ë³„ ë¹„êµ (í”¼ë²—)
                - 'model_comparison': ëª¨ë¸ë³„ ìµœê³  ì„±ëŠ¥
                - 'best_combinations': ìµœì  ì¡°í•© Top 5
        """
        if not self._evaluation_records:
            print("âš ï¸ No evaluation results found.")
            return {}

        df = pd.DataFrame(self._evaluation_records)

        report = {
            'full': df
        }

        # ì „ì²˜ë¦¬ë³„ ë¹„êµ (PA F1 ê¸°ì¤€)
        if 'pa_f1' in df.columns:
            report['preprocess_comparison'] = df.pivot_table(
                index='preprocess',
                columns='model',
                values='pa_f1',
                aggfunc='max'
            )

            # í›„ì²˜ë¦¬ë³„ ë¹„êµ
            report['postprocess_comparison'] = df.pivot_table(
                index='postprocess',
                columns='model',
                values='pa_f1',
                aggfunc='max'
            )

            # ëª¨ë¸ë³„ ìµœê³  ì„±ëŠ¥
            agg_dict = {
                'pa_f1': 'max',
                'point_f1': 'max',
            }
            # roc_auc, pr_aucëŠ” ìˆì„ ë•Œë§Œ ì¶”ê°€
            if 'roc_auc' in df.columns:
                agg_dict['roc_auc'] = 'max'
            if 'pr_auc' in df.columns:
                agg_dict['pr_auc'] = 'max'

            report['model_comparison'] = df.groupby('model').agg(agg_dict).round(4)

            # ìµœì  ì¡°í•© Top 5
            report['best_combinations'] = df.nlargest(5, 'pa_f1')[
                ['model', 'preprocess', 'postprocess', 'pa_f1', 'point_f1']
            ]

        return report

    def get_best(self, metric: str = 'pa_f1', model: Optional[str] = None) -> Dict:
        """
        ìµœì  ì¡°í•© ë°˜í™˜

        Args:
            metric: ê¸°ì¤€ ì§€í‘œ (default: 'pa_f1')
            model: íŠ¹ì • ëª¨ë¸ë§Œ í•„í„°ë§ (optional)

        Returns:
            ìµœì  ì¡°í•© ì •ë³´
        """
        if not self._evaluation_records:
            return {}

        df = pd.DataFrame(self._evaluation_records)

        if model:
            df = df[df['model'] == model]

        if metric not in df.columns:
            print(f"âš ï¸ Metric '{metric}' not found.")
            return {}

        best_idx = df[metric].idxmax()
        return df.loc[best_idx].to_dict()

    def print_summary(self):
        """í˜„ì¬ ì‹¤í—˜ í˜„í™© ì¶œë ¥"""
        print("=" * 60)
        print("ğŸ“Š Experiment Summary")
        print("=" * 60)

        print(f"\n[Training Records] {len(self._training_records)} experiments")
        if self._training_records:
            df = pd.DataFrame(self._training_records)
            print(df[['model', 'preprocess', 'dataset', 'training_time_sec']].to_string(index=False))

        print(f"\n[Evaluation Records] {len(self._evaluation_records)} experiments")
        if self._evaluation_records:
            df = pd.DataFrame(self._evaluation_records)
            cols = ['model', 'preprocess', 'postprocess', 'pa_f1']
            cols = [c for c in cols if c in df.columns]
            print(df[cols].to_string(index=False))

        print("\n" + "=" * 60)

    def export_to_excel(self, filename: str = 'experiment_results.xlsx'):
        """
        Excel íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° (ë³´ê³ ì„œìš©)

        Args:
            filename: ì¶œë ¥ íŒŒì¼ëª…
        """
        try:
            import openpyxl
        except ImportError:
            print("âš ï¸ openpyxl not installed. Exporting to CSV instead.")
            self._export_to_csv(filename.replace('.xlsx', ''))
            return

        filepath = self.results_dir / filename

        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
            # ì „ì²´ ê²°ê³¼
            if self._evaluation_records:
                pd.DataFrame(self._evaluation_records).to_excel(
                    writer, sheet_name='Full Results', index=False
                )

            # ë³´ê³ ì„œ í…Œì´ë¸”
            report = self.generate_report()
            for name, df in report.items():
                if isinstance(df, pd.DataFrame) and not df.empty:
                    df.to_excel(writer, sheet_name=name[:31])  # Excel ì‹œíŠ¸ëª… 31ì ì œí•œ

            # í•™ìŠµ ë¡œê·¸
            if self._training_records:
                pd.DataFrame(self._training_records).to_excel(
                    writer, sheet_name='Training Log', index=False
                )

        print(f"âœ… Exported to: {filepath}")

    def _export_to_csv(self, basename: str):
        """
        CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° (openpyxl ì—†ì„ ë•Œ ëŒ€ì²´ìš©)

        Args:
            basename: ì¶œë ¥ íŒŒì¼ ê¸°ë³¸ ì´ë¦„ (í™•ì¥ì ì œì™¸)
        """
        # ì „ì²´ í‰ê°€ ê²°ê³¼
        if self._evaluation_records:
            eval_path = self.results_dir / f'{basename}_evaluation.csv'
            pd.DataFrame(self._evaluation_records).to_csv(eval_path, index=False)
            print(f"âœ… Exported: {eval_path}")

        # í•™ìŠµ ë¡œê·¸
        if self._training_records:
            train_path = self.results_dir / f'{basename}_training.csv'
            pd.DataFrame(self._training_records).to_csv(train_path, index=False)
            print(f"âœ… Exported: {train_path}")
