# -*- coding: utf-8 -*-
"""
DLinear ëª¨ë¸ Wrapper
Prediction-based ì´ìƒì¹˜ íƒì§€ ëª¨ë¸

ğŸ“ [í•™ìŠµ í¬ì¸íŠ¸]
Prediction-based ì ‘ê·¼ë²•:
1. ì •ìƒ ë°ì´í„°ë¡œ "ë‹¤ìŒ ê°’ ì˜ˆì¸¡" ëª¨ë¸ í•™ìŠµ
2. í…ŒìŠ¤íŠ¸ ì‹œ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´(ì˜¤ì°¨)ë¥¼ ì´ìƒì¹˜ ì ìˆ˜ë¡œ ì‚¬ìš©
3. ì´ìƒì¹˜ = ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ íŒ¨í„´ = í° ì˜ˆì¸¡ ì˜¤ì°¨
"""

from typing import Optional, Literal

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

from .base import BaseModel
from .cores.dlinear import DLinear, DLinearConfig


class DLinearModel(BaseModel):
    """
    DLinear ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€ ëª¨ë¸

    Attributes:
        seq_len: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (lookback window)
        pred_len: ì˜ˆì¸¡ ê¸¸ì´ (ê¸°ë³¸ 1)
        lr: í•™ìŠµë¥ 
        epochs: í•™ìŠµ ì—í­ ìˆ˜
        batch_size: ë°°ì¹˜ í¬ê¸°
        device: 'cuda' ë˜ëŠ” 'cpu'
    """

    def __init__(
        self,
        seq_len: int = 100,
        pred_len: int = 1,
        lr: float = 0.001,
        epochs: int = 10,
        batch_size: int = 32,
        individual: bool = False,
        device: Optional[str] = None,
        early_stopping: bool = True,
        patience: int = 3,
        val_ratio: float = 0.1,
    ):
        """
        Args:
            seq_len: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (lookback window)
            pred_len: ì˜ˆì¸¡ ê¸¸ì´ (ê¸°ë³¸ 1)
            lr: í•™ìŠµë¥ 
            epochs: ìµœëŒ€ í•™ìŠµ ì—í­ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            individual: ë³€ìˆ˜ë³„ ë…ë¦½ í•™ìŠµ ì—¬ë¶€
            device: 'cuda' ë˜ëŠ” 'cpu'
            early_stopping: Early Stopping í™œì„±í™” ì—¬ë¶€
            patience: Early Stopping patience (ì—°ì† në²ˆ ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ)
            val_ratio: Validation ë°ì´í„° ë¹„ìœ¨
        """
        super().__init__()
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.lr = lr
        self.epochs = epochs
        self.batch_size = batch_size
        self.individual = individual

        # Early Stopping ì„¤ì •
        self.early_stopping = early_stopping
        self.patience = patience
        self.val_ratio = val_ratio

        # Device ì„¤ì •
        if device is None:
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device

        self.model: Optional[DLinear] = None
        self.n_features: Optional[int] = None
        self.best_val_loss: float = float('inf')
        self.best_model_state: Optional[dict] = None

    def _create_sequences(self, data: np.ndarray) -> tuple:
        """
        ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì‹œí€€ìŠ¤ ìƒì„±

        Args:
            data: (n_samples, n_features)

        Returns:
            X: (n_sequences, seq_len, n_features)
            y: (n_sequences, pred_len, n_features)
        """
        # ============================================================
        # TODO(human): ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì‹œí€€ìŠ¤ ìƒì„±
        # ============================================================
        # ì‹œê³„ì—´ì„ (ì…ë ¥, íƒ€ê²Ÿ) ìŒìœ¼ë¡œ ë³€í™˜
        # ì˜ˆ: seq_len=3, pred_len=1ì¼ ë•Œ
        #     data = [0,1,2,3,4,5]
        #     X[0] = [0,1,2], y[0] = [3]
        #     X[1] = [1,2,3], y[1] = [4]
        #     ...
        #
        # Hint: ë°˜ë³µë¬¸ìœ¼ë¡œ ìœˆë„ìš°ë¥¼ ì´ë™í•˜ë©° ìŠ¬ë¼ì´ì‹±

        X, y = [], []

        for i in range(len(data) - self.seq_len - self.pred_len + 1):
            X.append(data[i:i + self.seq_len])
            y.append(data[i + self.seq_len:i + self.seq_len + self.pred_len])

        if len(X) == 0:
            raise NotImplementedError(
                "TODO(human): _create_sequences()ì˜ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¥¼ êµ¬í˜„í•´ì£¼ì„¸ìš”!"
            )

        return np.array(X), np.array(y)

    def fit(self, train_data: np.ndarray, verbose: bool = True, **kwargs) -> 'DLinearModel':
        """
        DLinear ëª¨ë¸ í•™ìŠµ (Early Stopping ì§€ì›)

        Args:
            train_data: í•™ìŠµ ë°ì´í„° (n_samples, n_features)
            verbose: í•™ìŠµ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.n_features = train_data.shape[1]

        # ëª¨ë¸ ìƒì„±
        config = DLinearConfig(
            seq_len=self.seq_len,
            pred_len=self.pred_len,
            enc_in=self.n_features,
            individual=self.individual,
        )
        self.model = DLinear(config).to(self.device)

        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = self._create_sequences(train_data)

        # Train/Validation ë¶„í•  (Early Stoppingìš©)
        if self.early_stopping and self.val_ratio > 0:
            n_val = int(len(X) * self.val_ratio)
            X_train, X_val = X[:-n_val], X[-n_val:]
            y_train, y_val = y[:-n_val], y[-n_val:]

            X_train_tensor = torch.FloatTensor(X_train).to(self.device)
            y_train_tensor = torch.FloatTensor(y_train).to(self.device)
            X_val_tensor = torch.FloatTensor(X_val).to(self.device)
            y_val_tensor = torch.FloatTensor(y_val).to(self.device)
        else:
            X_train_tensor = torch.FloatTensor(X).to(self.device)
            y_train_tensor = torch.FloatTensor(y).to(self.device)
            X_val_tensor, y_val_tensor = None, None

        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)

        # í•™ìŠµ
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)

        # Early Stopping ë³€ìˆ˜
        self.best_val_loss = float('inf')
        self.best_model_state = None
        patience_counter = 0

        for epoch in range(self.epochs):
            # Training
            self.model.train()
            total_loss = 0
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                output = self.model(batch_X)
                loss = criterion(output, batch_y)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            avg_train_loss = total_loss / len(train_loader)

            # Validation (Early Stopping)
            if self.early_stopping and X_val_tensor is not None:
                self.model.eval()
                with torch.no_grad():
                    val_output = self.model(X_val_tensor)
                    val_loss = criterion(val_output, y_val_tensor).item()

                # Best model ì €ì¥
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.best_model_state = self.model.state_dict().copy()
                    patience_counter = 0
                else:
                    patience_counter += 1

                if verbose and (epoch + 1) % max(1, self.epochs // 5) == 0:
                    print(f"Epoch [{epoch+1}/{self.epochs}] Train: {avg_train_loss:.6f}, Val: {val_loss:.6f}")

                # Early Stopping ì²´í¬
                if patience_counter >= self.patience:
                    if verbose:
                        print(f"  âš¡ Early stopping at epoch {epoch+1} (patience={self.patience})")
                    break
            else:
                if verbose and (epoch + 1) % max(1, self.epochs // 5) == 0:
                    print(f"Epoch [{epoch+1}/{self.epochs}] Loss: {avg_train_loss:.6f}")

        # Best model ë³µì›
        if self.early_stopping and self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
            if verbose:
                print(f"  âœ“ Best model restored (val_loss: {self.best_val_loss:.6f})")

        self._is_fitted = True
        return self

    def predict(self, data: np.ndarray) -> np.ndarray:
        """
        ì˜ˆì¸¡ ìˆ˜í–‰

        Args:
            data: ì…ë ¥ ë°ì´í„° (n_samples, n_features)

        Returns:
            ì˜ˆì¸¡ê°’ (n_valid_samples, pred_len, n_features)
        """
        self._check_is_fitted()

        X, _ = self._create_sequences(data)
        X_tensor = torch.FloatTensor(X).to(self.device)

        self.model.eval()
        with torch.no_grad():
            predictions = self.model(X_tensor)

        return predictions.cpu().numpy()

    def get_anomaly_score(self, data: np.ndarray) -> np.ndarray:
        """
        ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚°

        ğŸ“ [í•™ìŠµ í¬ì¸íŠ¸]
        Prediction-based ì´ìƒì¹˜ ì ìˆ˜:
        score = |ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’|ì˜ í‰ê·  (ê° ì‹œì ë³„)

        Args:
            data: ì…ë ¥ ë°ì´í„° (n_samples, n_features)

        Returns:
            ì´ìƒì¹˜ ì ìˆ˜ (n_samples,)
        """
        self._check_is_fitted()

        X, y = self._create_sequences(data)

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¶”ë¡  (ë©”ëª¨ë¦¬ íš¨ìœ¨)
        self.model.eval()
        batch_size = 1024
        predictions_list = []

        with torch.no_grad():
            for i in range(0, len(X), batch_size):
                batch_X = torch.FloatTensor(X[i:i+batch_size]).to(self.device)
                batch_pred = self.model(batch_X).cpu().numpy()
                predictions_list.append(batch_pred)
                del batch_X  # GPU ë©”ëª¨ë¦¬ í•´ì œ
                torch.cuda.empty_cache()

        predictions = np.concatenate(predictions_list, axis=0)

        # ============================================================
        # TODO(human): ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚°
        # ============================================================
        # predictions: (n_sequences, pred_len, n_features)
        # y: (n_sequences, pred_len, n_features)
        #
        # ì´ìƒì¹˜ ì ìˆ˜ = |y - predictions|ì˜ feature í‰ê· 
        # ê²°ê³¼ shape: (n_sequences,) ë˜ëŠ” (n_sequences, pred_len)
        #
        # Hint: np.abs(), np.mean(axis=...)

        # ì˜ˆì¸¡ ì˜¤ì°¨ ê³„ì‚° (MAE)
        errors = np.abs(y - predictions)  # (n_seq, pred_len, n_features)

        scores = scores = np.mean(errors, axis=(1, 2))  # (n_sequences,)

        if scores is None:
            raise NotImplementedError(
                "TODO(human): get_anomaly_score()ì˜ ì ìˆ˜ ê³„ì‚°ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”!"
            )

        # ì „ì²´ ë°ì´í„° ê¸¸ì´ì— ë§ê²Œ íŒ¨ë”© (ì•ë¶€ë¶„ì€ ì ìˆ˜ ê³„ì‚° ë¶ˆê°€)
        full_scores = np.zeros(len(data))
        full_scores[self.seq_len:self.seq_len + len(scores)] = scores

        return full_scores
