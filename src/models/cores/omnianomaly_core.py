# -*- coding: utf-8 -*-
"""
OmniAnomaly Core Implementation (PyTorch)

Original: https://github.com/NetManAIOps/OmniAnomaly
Paper: "Robust Anomaly Detection for Multivariate Time Series" (KDD 2019)

ğŸ“ [í•™ìŠµ í¬ì¸íŠ¸]
OmniAnomalyëŠ” Reconstruction-based ì´ìƒ íƒì§€ ëª¨ë¸ë¡œ:
1. GRU ê¸°ë°˜ ì¸ì½”ë”ë¡œ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
2. VAEë¡œ ì ì¬ ê³µê°„ì—ì„œ ì •ìƒ íŒ¨í„´ ëª¨ë¸ë§
3. Planar Normalizing Flowë¡œ ì‚¬í›„ ë¶„í¬ í‘œí˜„ë ¥ í–¥ìƒ
4. ë³µì› í™•ë¥ (reconstruction probability)ë¡œ ì´ìƒ ì ìˆ˜ ê³„ì‚°

Architecture:
    x â†’ GRU Encoder â†’ q(z|x) â†’ z â†’ GRU Decoder â†’ p(x|z) â†’ x_reconstructed
                        â†“
                  Normalizing Flow (optional)
"""

import math
from typing import Dict, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal, kl_divergence


class GRUEncoder(nn.Module):
    """
    GRU ê¸°ë°˜ ì¸ì½”ë”

    ì‹œê³„ì—´ ì…ë ¥ì„ ë°›ì•„ ì ì¬ ë³€ìˆ˜ zì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        x_dim: int,
        z_dim: int,
        hidden_dim: int = 100,
        num_layers: int = 1,
        dropout: float = 0.0
    ):
        super().__init__()
        self.x_dim = x_dim
        self.z_dim = z_dim
        self.hidden_dim = hidden_dim

        self.gru = nn.GRU(
            input_size=x_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # zì˜ í‰ê· ê³¼ ë¡œê·¸ ë¶„ì‚°ì„ ì¶œë ¥í•˜ëŠ” ë ˆì´ì–´
        self.fc_mu = nn.Linear(hidden_dim, z_dim)
        self.fc_logvar = nn.Linear(hidden_dim, z_dim)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: (batch, seq_len, x_dim)

        Returns:
            mu: (batch, seq_len, z_dim) - ì ì¬ ë³€ìˆ˜ í‰ê· 
            logvar: (batch, seq_len, z_dim) - ì ì¬ ë³€ìˆ˜ ë¡œê·¸ ë¶„ì‚°
        """
        # GRU ì¸ì½”ë”©
        h, _ = self.gru(x)  # (batch, seq_len, hidden_dim)

        # í‰ê· ê³¼ ë¡œê·¸ ë¶„ì‚° ê³„ì‚°
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)

        return mu, logvar


class GRUDecoder(nn.Module):
    """
    GRU ê¸°ë°˜ ë””ì½”ë”

    ì ì¬ ë³€ìˆ˜ zë¥¼ ë°›ì•„ ì›ë³¸ xë¥¼ ë³µì›í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        x_dim: int,
        z_dim: int,
        hidden_dim: int = 100,
        num_layers: int = 1,
        dropout: float = 0.0
    ):
        super().__init__()
        self.x_dim = x_dim
        self.z_dim = z_dim
        self.hidden_dim = hidden_dim

        self.gru = nn.GRU(
            input_size=z_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # ë³µì›ëœ xì˜ í‰ê· ê³¼ ë¡œê·¸ ë¶„ì‚°
        self.fc_mu = nn.Linear(hidden_dim, x_dim)
        self.fc_logvar = nn.Linear(hidden_dim, x_dim)

    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            z: (batch, seq_len, z_dim)

        Returns:
            x_mu: (batch, seq_len, x_dim) - ë³µì›ëœ xì˜ í‰ê· 
            x_logvar: (batch, seq_len, x_dim) - ë³µì›ëœ xì˜ ë¡œê·¸ ë¶„ì‚°
        """
        h, _ = self.gru(z)  # (batch, seq_len, hidden_dim)

        x_mu = self.fc_mu(h)
        x_logvar = self.fc_logvar(h)

        return x_mu, x_logvar


class PlanarFlow(nn.Module):
    """
    Planar Normalizing Flow

    ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ Normalizing Flow êµ¬í˜„.
    z' = z + u * tanh(w^T * z + b)

    ğŸ“ [í•™ìŠµ í¬ì¸íŠ¸]
    - Normalizing Flow: ë‹¨ìˆœí•œ ë¶„í¬ë¥¼ ë³µì¡í•œ ë¶„í¬ë¡œ ë³€í™˜
    - Planar Flow: í‰ë©´(hyperplane)ì„ ë”°ë¼ ë¶„í¬ë¥¼ ë³€í˜•
    - ì—¬ëŸ¬ ì¸µì„ ìŒ“ì•„ í‘œí˜„ë ¥ í–¥ìƒ
    """

    def __init__(self, z_dim: int):
        super().__init__()
        self.z_dim = z_dim

        # Planar flow íŒŒë¼ë¯¸í„°
        self.u = nn.Parameter(torch.randn(z_dim) * 0.01)
        self.w = nn.Parameter(torch.randn(z_dim) * 0.01)
        self.b = nn.Parameter(torch.zeros(1))

    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            z: (batch, ..., z_dim)

        Returns:
            z': ë³€í™˜ëœ z
            log_det: ë¡œê·¸ ì•¼ì½”ë¹„ì•ˆ í–‰ë ¬ì‹
        """
        # w^T * z + b
        linear = F.linear(z, self.w.unsqueeze(0), self.b)  # (..., 1)
        linear = linear.squeeze(-1)  # (...)

        # tanh activation
        tanh_linear = torch.tanh(linear)

        # z' = z + u * tanh(w^T * z + b)
        z_new = z + self.u * tanh_linear.unsqueeze(-1)

        # ë¡œê·¸ ì•¼ì½”ë¹„ì•ˆ: log|1 + u^T * w * (1 - tanh^2)|
        psi = (1 - tanh_linear ** 2).unsqueeze(-1) * self.w  # (..., z_dim)
        det = 1 + (psi * self.u).sum(dim=-1)
        log_det = torch.log(torch.abs(det) + 1e-8)

        return z_new, log_det


class NormalizingFlows(nn.Module):
    """ì—¬ëŸ¬ Planar Flow ì¸µì„ ìŒ“ì€ ëª¨ë“ˆ"""

    def __init__(self, z_dim: int, n_flows: int = 2):
        super().__init__()
        self.flows = nn.ModuleList([PlanarFlow(z_dim) for _ in range(n_flows)])

    def forward(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Returns:
            z_k: ìµœì¢… ë³€í™˜ëœ z
            sum_log_det: ëˆ„ì  ë¡œê·¸ ì•¼ì½”ë¹„ì•ˆ
        """
        sum_log_det = 0
        z_k = z

        for flow in self.flows:
            z_k, log_det = flow(z_k)
            sum_log_det = sum_log_det + log_det

        return z_k, sum_log_det


class OmniAnomalyCore(nn.Module):
    """
    OmniAnomaly í•µì‹¬ ëª¨ë¸ (PyTorch ë²„ì „)

    êµ¬ì„± ìš”ì†Œ:
    1. GRU Encoder: x â†’ (mu, logvar)
    2. Reparameterization: z = mu + std * epsilon
    3. (Optional) Normalizing Flow: z â†’ z'
    4. GRU Decoder: z' â†’ (x_mu, x_logvar)

    ì†ì‹¤ í•¨ìˆ˜: ELBO = E[log p(x|z)] - KL(q(z|x) || p(z))

    ğŸ“ [í•™ìŠµ í¬ì¸íŠ¸]
    - VAEì˜ í•µì‹¬: ì ì¬ ê³µê°„ì—ì„œ ì •ìƒ íŒ¨í„´ í•™ìŠµ
    - ì´ìƒ ë°ì´í„°ëŠ” ì ì¬ ê³µê°„ì—ì„œ ì˜ í‘œí˜„ë˜ì§€ ì•ŠìŒ â†’ ë³µì› ì˜¤ë¥˜ ì¦ê°€
    - Normalizing Flowë¡œ ì‚¬í›„ ë¶„í¬ì˜ í‘œí˜„ë ¥ í–¥ìƒ
    """

    def __init__(
        self,
        x_dim: int,
        z_dim: int = 8,
        hidden_dim: int = 100,
        num_layers: int = 1,
        n_flows: int = 2,
        use_flow: bool = True,
        dropout: float = 0.0
    ):
        """
        Args:
            x_dim: ì…ë ¥ ì°¨ì› (feature ìˆ˜)
            z_dim: ì ì¬ ê³µê°„ ì°¨ì›
            hidden_dim: GRU hidden ì°¨ì›
            num_layers: GRU ì¸µ ìˆ˜
            n_flows: Normalizing Flow ì¸µ ìˆ˜
            use_flow: Normalizing Flow ì‚¬ìš© ì—¬ë¶€
            dropout: Dropout ë¹„ìœ¨
        """
        super().__init__()

        self.x_dim = x_dim
        self.z_dim = z_dim
        self.hidden_dim = hidden_dim
        self.use_flow = use_flow

        # Encoder & Decoder
        self.encoder = GRUEncoder(x_dim, z_dim, hidden_dim, num_layers, dropout)
        self.decoder = GRUDecoder(x_dim, z_dim, hidden_dim, num_layers, dropout)

        # Normalizing Flow (optional)
        if use_flow and n_flows > 0:
            self.flow = NormalizingFlows(z_dim, n_flows)
        else:
            self.flow = None

    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        """
        Reparameterization trick

        z = mu + std * epsilon, where epsilon ~ N(0, I)

        ğŸ“ [í•™ìŠµ í¬ì¸íŠ¸]
        - VAEì—ì„œ gradientê°€ samplingì„ í†µê³¼í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” í•µì‹¬ ê¸°ë²•
        - mu, logvarëŠ” í•™ìŠµ ê°€ëŠ¥, epsilonì€ ë…¸ì´ì¦ˆ
        """
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ì¸ì½”ë”©: x â†’ z

        Args:
            x: (batch, seq_len, x_dim)

        Returns:
            z: ìƒ˜í”Œë§ëœ ì ì¬ ë³€ìˆ˜
            mu: ì ì¬ ë³€ìˆ˜ í‰ê· 
            logvar: ì ì¬ ë³€ìˆ˜ ë¡œê·¸ ë¶„ì‚°
        """
        mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logvar)
        return z, mu, logvar

    def decode(self, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ë””ì½”ë”©: z â†’ x_reconstructed

        Args:
            z: (batch, seq_len, z_dim)

        Returns:
            x_mu: ë³µì›ëœ xì˜ í‰ê· 
            x_logvar: ë³µì›ëœ xì˜ ë¡œê·¸ ë¶„ì‚°
        """
        return self.decoder(z)

    def forward(
        self,
        x: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        ìˆœì „íŒŒ

        Args:
            x: (batch, seq_len, x_dim)

        Returns:
            dict with:
                - x_mu: ë³µì›ëœ xì˜ í‰ê· 
                - x_logvar: ë³µì›ëœ xì˜ ë¡œê·¸ ë¶„ì‚°
                - z: ì ì¬ ë³€ìˆ˜
                - z_mu: ì ì¬ ë³€ìˆ˜ í‰ê· 
                - z_logvar: ì ì¬ ë³€ìˆ˜ ë¡œê·¸ ë¶„ì‚°
                - flow_log_det: Normalizing Flow ë¡œê·¸ ì•¼ì½”ë¹„ì•ˆ (ì‚¬ìš© ì‹œ)
        """
        # Encode
        z, z_mu, z_logvar = self.encode(x)

        # Apply Normalizing Flow (optional)
        flow_log_det = None
        if self.flow is not None:
            z, flow_log_det = self.flow(z)

        # Decode
        x_mu, x_logvar = self.decode(z)

        return {
            'x_mu': x_mu,
            'x_logvar': x_logvar,
            'z': z,
            'z_mu': z_mu,
            'z_logvar': z_logvar,
            'flow_log_det': flow_log_det
        }

    def compute_loss(
        self,
        x: torch.Tensor,
        outputs: Dict[str, torch.Tensor],
        beta: float = 1.0
    ) -> Dict[str, torch.Tensor]:
        """
        ELBO ì†ì‹¤ ê³„ì‚°

        Loss = -E[log p(x|z)] + beta * KL(q(z|x) || p(z))

        Args:
            x: ì›ë³¸ ì…ë ¥
            outputs: forward() ì¶œë ¥
            beta: KL divergence ê°€ì¤‘ì¹˜ (beta-VAE)

        Returns:
            dict with loss components
        """
        x_mu = outputs['x_mu']
        x_logvar = outputs['x_logvar']
        z_mu = outputs['z_mu']
        z_logvar = outputs['z_logvar']
        flow_log_det = outputs.get('flow_log_det')

        # Reconstruction Loss: Negative log-likelihood under Gaussian
        # -log p(x|z) = 0.5 * (log(2Ï€) + logvar + (x - mu)^2 / var)
        # ìƒìˆ˜í•­ log(2Ï€)ëŠ” ìµœì í™”ì— ì˜í–¥ ì—†ìœ¼ë¯€ë¡œ ìƒëµ ê°€ëŠ¥
        x_std = torch.exp(0.5 * x_logvar)
        dist = Normal(x_mu, x_std)
        log_prob = dist.log_prob(x)  # (batch, seq_len, x_dim)
        recon_loss = -log_prob.sum(dim=-1).mean()  # negative log prob, sum over features

        # KL Divergence: KL(q(z|x) || p(z)), where p(z) = N(0, I)
        # KL = -0.5 * sum(1 + logvar - mu^2 - exp(logvar))
        kl_loss = -0.5 * (1 + z_logvar - z_mu.pow(2) - z_logvar.exp())
        kl_loss = kl_loss.sum(dim=-1).mean()

        # Normalizing Flow ë³´ì • (ì‚¬ìš© ì‹œ)
        if flow_log_det is not None:
            kl_loss = kl_loss - flow_log_det.mean()

        # Total ELBO loss
        total_loss = recon_loss + beta * kl_loss

        return {
            'loss': total_loss,
            'recon_loss': recon_loss,
            'kl_loss': kl_loss
        }

    def get_reconstruction_prob(
        self,
        x: torch.Tensor,
        n_samples: int = 1
    ) -> torch.Tensor:
        """
        ë³µì› í™•ë¥  ê³„ì‚° (ì´ìƒ ì ìˆ˜ì˜ ê¸°ë°˜)

        ì—¬ëŸ¬ ë²ˆ ìƒ˜í”Œë§í•˜ì—¬ í‰ê·  ë³µì› í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ì´ìƒì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.

        Args:
            x: (batch, seq_len, x_dim)
            n_samples: z ìƒ˜í”Œ ìˆ˜

        Returns:
            recon_prob: (batch, seq_len) - ê° ì‹œì ì˜ ë³µì› í™•ë¥ 
        """
        total_log_prob = 0

        for _ in range(n_samples):
            outputs = self.forward(x)
            x_mu = outputs['x_mu']
            x_logvar = outputs['x_logvar']

            # Log probability under Gaussian
            x_std = torch.exp(0.5 * x_logvar)
            dist = Normal(x_mu, x_std)
            log_prob = dist.log_prob(x)  # (batch, seq_len, x_dim)
            log_prob = log_prob.sum(dim=-1)  # (batch, seq_len)

            total_log_prob = total_log_prob + log_prob

        # í‰ê· 
        mean_log_prob = total_log_prob / n_samples

        return mean_log_prob

    def get_anomaly_score(
        self,
        x: torch.Tensor,
        n_samples: int = 1,
        last_point_only: bool = True
    ) -> torch.Tensor:
        """
        ì´ìƒ ì ìˆ˜ ê³„ì‚°

        ì´ìƒ ì ìˆ˜ = -ë³µì› í™•ë¥  (í´ìˆ˜ë¡ ì´ìƒ)

        Args:
            x: (batch, seq_len, x_dim)
            n_samples: z ìƒ˜í”Œ ìˆ˜
            last_point_only: ë§ˆì§€ë§‰ ì‹œì ë§Œ ë°˜í™˜í• ì§€ ì—¬ë¶€

        Returns:
            scores: ì´ìƒ ì ìˆ˜ (í´ìˆ˜ë¡ ì´ìƒ)
        """
        recon_prob = self.get_reconstruction_prob(x, n_samples)

        # ìŒì˜ ë³µì› í™•ë¥  = ì´ìƒ ì ìˆ˜
        scores = -recon_prob

        if last_point_only:
            scores = scores[:, -1]  # (batch,)

        return scores
