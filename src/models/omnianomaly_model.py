# -*- coding: utf-8 -*-
"""
OmniAnomaly Model Wrapper

BaseModel ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

ğŸ“ [í•™ìŠµ í¬ì¸íŠ¸]
- Reconstruction-based ì´ìƒ íƒì§€: ì •ìƒ íŒ¨í„´ ë³µì› í•™ìŠµ â†’ ì´ìƒ ë°ì´í„°ëŠ” ë³µì› ì˜¤ë¥˜ ì¦ê°€
- VAEì˜ ELBO ì†ì‹¤: Reconstruction + KL Divergence
- ì´ìƒ ì ìˆ˜: -log p(x|z), ë³µì› í™•ë¥ ì´ ë‚®ì„ìˆ˜ë¡ ì´ìƒ
"""

from typing import Dict, Optional

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm

from .base import BaseModel
from .cores.omnianomaly_core import OmniAnomalyCore


class OmniAnomaly(BaseModel):
    """
    OmniAnomaly Wrapper (Reconstruction-based)

    ì›ë³¸ ë…¼ë¬¸: "Robust Anomaly Detection for Multivariate Time Series" (KDD 2019)

    í•µì‹¬ êµ¬ì¡°:
    - GRU ê¸°ë°˜ VAE
    - Planar Normalizing Flowë¡œ ì‚¬í›„ ë¶„í¬ í‘œí˜„ë ¥ í–¥ìƒ
    - ë³µì› í™•ë¥  ê¸°ë°˜ ì´ìƒ ì ìˆ˜

    ğŸ“ [í•™ìŠµ í¬ì¸íŠ¸]
    - DLinear(Prediction)ì™€ ë‹¬ë¦¬ 'ë³µì›'ìœ¼ë¡œ ì´ìƒ íƒì§€
    - ì •ìƒ ë°ì´í„°ë¡œë§Œ í•™ìŠµ â†’ ì´ìƒ ë°ì´í„°ëŠ” ì˜ ë³µì›ë˜ì§€ ì•ŠìŒ
    - Multi-modal ë¶„í¬ë„ Normalizing Flowë¡œ í‘œí˜„ ê°€ëŠ¥
    """

    def __init__(self, config: Dict):
        """
        Args:
            config: ëª¨ë¸ ì„¤ì •
                - x_dim: ì…ë ¥ ì°¨ì› (feature ìˆ˜)
                - z_dim: ì ì¬ ê³µê°„ ì°¨ì› (default: 8)
                - hidden_dim: GRU hidden ì°¨ì› (default: 100)
                - window_length: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (default: 100)
                - n_flows: Normalizing Flow ì¸µ ìˆ˜ (default: 2)
                - use_flow: Flow ì‚¬ìš© ì—¬ë¶€ (default: True)
                - epochs: ìµœëŒ€ í•™ìŠµ ì—í­ (default: 50)
                - batch_size: ë°°ì¹˜ í¬ê¸° (default: 256)
                - learning_rate: í•™ìŠµë¥  (default: 1e-3)
                - beta: KL ê°€ì¤‘ì¹˜ (default: 1.0)
                - device: 'cuda' or 'cpu' (default: auto)
                - early_stopping: Early Stopping í™œì„±í™” (default: True)
                - patience: Early Stopping patience (default: 5)
                - val_ratio: Validation ë¹„ìœ¨ (default: 0.1)
        """
        super().__init__()
        self.config = config

        # ê¸°ë³¸ ì„¤ì • (ë…¼ë¬¸ ì›ë³¸ ê°’)
        self.x_dim = config.get('x_dim', None)  # fitì—ì„œ ìë™ ì„¤ì •
        self.z_dim = config.get('z_dim', 3)           # ë…¼ë¬¸: 3
        self.hidden_dim = config.get('hidden_dim', 500)  # ë…¼ë¬¸: 500
        self.window_length = config.get('window_length', 100)  # ë…¼ë¬¸: 100
        self.n_flows = config.get('n_flows', 20)      # ë…¼ë¬¸: 20 (Planar NF)
        self.use_flow = config.get('use_flow', True)

        # í•™ìŠµ ì„¤ì • (ë…¼ë¬¸ ì›ë³¸ ê°’)
        self.epochs = config.get('epochs', 20)        # ë…¼ë¬¸: 20
        self.batch_size = config.get('batch_size', 50)  # ë…¼ë¬¸: 50
        self.learning_rate = config.get('learning_rate', 1e-3)  # ë…¼ë¬¸: 1e-3
        self.beta = config.get('beta', 1.0)
        self.n_samples = config.get('n_samples', 1)  # ì´ìƒ ì ìˆ˜ ê³„ì‚° ì‹œ ìƒ˜í”Œ ìˆ˜
        self.weight_decay = config.get('weight_decay', 1e-4)  # ë…¼ë¬¸: L2 reg 1e-4

        # Early Stopping ì„¤ì • (ì›ë³¸ OmniAnomalyì—ë„ early_stop=True ì˜µì…˜ ìˆìŒ)
        self.early_stopping = config.get('early_stopping', True)
        self.patience = config.get('patience', 5)
        self.val_ratio = config.get('val_ratio', 0.3)  # ë…¼ë¬¸: 30%

        # Device ì„¤ì •
        if config.get('device'):
            self.device = torch.device(config['device'])
        else:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        self.model: Optional[OmniAnomalyCore] = None
        self.best_val_loss: float = float('inf')
        self.best_model_state: Optional[dict] = None

    def _create_windows(self, data: np.ndarray) -> np.ndarray:
        """
        ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì‹œí€€ìŠ¤ ìƒì„±

        Args:
            data: (n_samples, n_features)

        Returns:
            windows: (n_windows, window_length, n_features)
        """
        n_samples = len(data)
        n_windows = n_samples - self.window_length + 1

        if n_windows <= 0:
            raise ValueError(
                f"Data length ({n_samples}) is shorter than window_length ({self.window_length})"
            )

        windows = np.array([
            data[i:i + self.window_length]
            for i in range(n_windows)
        ])

        return windows

    def fit(self, train_data: np.ndarray, verbose: bool = True) -> 'OmniAnomaly':
        """
        ëª¨ë¸ í•™ìŠµ (Early Stopping ì§€ì›)

        Args:
            train_data: (n_samples, n_features) í•™ìŠµ ë°ì´í„°
            verbose: í•™ìŠµ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€

        Returns:
            self
        """
        # x_dim ìë™ ì„¤ì •
        if self.x_dim is None:
            self.x_dim = train_data.shape[1]

        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = OmniAnomalyCore(
            x_dim=self.x_dim,
            z_dim=self.z_dim,
            hidden_dim=self.hidden_dim,
            n_flows=self.n_flows,
            use_flow=self.use_flow
        ).to(self.device)

        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±
        windows = self._create_windows(train_data)

        # Train/Validation ë¶„í•  (Early Stoppingìš©)
        if self.early_stopping and self.val_ratio > 0:
            n_val = int(len(windows) * self.val_ratio)
            train_windows = windows[:-n_val]
            val_windows = windows[-n_val:]

            train_tensor = torch.FloatTensor(train_windows).to(self.device)
            val_tensor = torch.FloatTensor(val_windows).to(self.device)
        else:
            train_tensor = torch.FloatTensor(windows).to(self.device)
            val_tensor = None

        # DataLoader
        train_dataset = TensorDataset(train_tensor)
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            drop_last=True
        )

        # Optimizer (ë…¼ë¬¸: Adam + L2 regularization)
        optimizer = torch.optim.Adam(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay  # L2 ì •ê·œí™” (ë…¼ë¬¸: 1e-4)
        )

        # Early Stopping ë³€ìˆ˜
        self.best_val_loss = float('inf')
        self.best_model_state = None
        patience_counter = 0

        # í•™ìŠµ ë£¨í”„
        history = {'loss': [], 'recon_loss': [], 'kl_loss': [], 'val_loss': []}

        for epoch in range(self.epochs):
            # Training
            self.model.train()
            epoch_loss = 0
            epoch_recon = 0
            epoch_kl = 0
            n_batches = 0

            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{self.epochs}', leave=False, disable=not verbose)
            for batch in pbar:
                x = batch[0]

                optimizer.zero_grad()

                # Forward
                outputs = self.model(x)

                # Loss
                losses = self.model.compute_loss(x, outputs, beta=self.beta)
                loss = losses['loss']

                # Backward
                loss.backward()

                # Gradient Clipping (ìˆ˜ì¹˜ ì•ˆì •ì„±)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)

                optimizer.step()

                # ê¸°ë¡
                epoch_loss += losses['loss'].item()
                epoch_recon += losses['recon_loss'].item()
                epoch_kl += losses['kl_loss'].item()
                n_batches += 1

                pbar.set_postfix({
                    'loss': f"{losses['loss'].item():.4f}",
                    'recon': f"{losses['recon_loss'].item():.4f}",
                    'kl': f"{losses['kl_loss'].item():.4f}"
                })

            # Epoch í‰ê· 
            avg_train_loss = epoch_loss / n_batches
            history['loss'].append(avg_train_loss)
            history['recon_loss'].append(epoch_recon / n_batches)
            history['kl_loss'].append(epoch_kl / n_batches)

            # Validation (Early Stopping)
            if self.early_stopping and val_tensor is not None:
                self.model.eval()
                with torch.no_grad():
                    # Validation loss ê³„ì‚° (ë°°ì¹˜ë¡œ ë¶„í• )
                    val_loss_total = 0
                    val_batches = 0
                    for i in range(0, len(val_tensor), self.batch_size):
                        val_batch = val_tensor[i:i + self.batch_size]
                        if len(val_batch) < 2:  # ë„ˆë¬´ ì‘ì€ ë°°ì¹˜ ìŠ¤í‚µ
                            continue
                        val_outputs = self.model(val_batch)
                        val_losses = self.model.compute_loss(val_batch, val_outputs, beta=self.beta)
                        val_loss_total += val_losses['loss'].item()
                        val_batches += 1

                    val_loss = val_loss_total / max(val_batches, 1)
                    history['val_loss'].append(val_loss)

                # Best model ì €ì¥
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.best_model_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}
                    patience_counter = 0
                else:
                    patience_counter += 1

                if verbose and (epoch + 1) % 5 == 0:
                    print(f"Epoch {epoch+1}: Train={avg_train_loss:.4f}, Val={val_loss:.4f}")

                # Early Stopping ì²´í¬
                if patience_counter >= self.patience:
                    if verbose:
                        print(f"  âš¡ Early stopping at epoch {epoch+1} (patience={self.patience})")
                    break
            else:
                if verbose and (epoch + 1) % 10 == 0:
                    print(f"Epoch {epoch+1}: Loss={avg_train_loss:.4f}, "
                          f"Recon={history['recon_loss'][-1]:.4f}, KL={history['kl_loss'][-1]:.4f}")

        # Best model ë³µì›
        if self.early_stopping and self.best_model_state is not None:
            self.model.load_state_dict(self.best_model_state)
            if verbose:
                print(f"  âœ“ Best model restored (val_loss: {self.best_val_loss:.4f})")

        self.is_fitted = True
        self.history = history
        return self

    def predict(self, test_data: np.ndarray) -> np.ndarray:
        """
        ë³µì›ëœ ë°ì´í„° ë°˜í™˜

        Args:
            test_data: (n_samples, n_features)

        Returns:
            reconstructed: (n_samples, n_features)
        """
        if not self.is_fitted:
            raise RuntimeError("Model not fitted. Call fit() first.")

        self.model.eval()

        # ìœˆë„ìš° ìƒì„±
        windows = self._create_windows(test_data)
        windows_tensor = torch.FloatTensor(windows).to(self.device)

        reconstructed_list = []

        with torch.no_grad():
            for i in range(0, len(windows_tensor), self.batch_size):
                batch = windows_tensor[i:i + self.batch_size]
                outputs = self.model(batch)

                # ë§ˆì§€ë§‰ ì‹œì ì˜ ë³µì›ê°’ ì‚¬ìš©
                x_mu = outputs['x_mu'][:, -1, :]  # (batch, x_dim)
                reconstructed_list.append(x_mu.cpu().numpy())

        reconstructed = np.concatenate(reconstructed_list, axis=0)

        # ì²« window_length-1 ê°œëŠ” íŒ¨ë”© (ì²« ë²ˆì§¸ ë³µì›ê°’ìœ¼ë¡œ)
        padding = np.tile(reconstructed[0], (self.window_length - 1, 1))
        reconstructed = np.vstack([padding, reconstructed])

        return reconstructed

    def get_anomaly_score(self, test_data: np.ndarray) -> np.ndarray:
        """
        ì´ìƒ ì ìˆ˜ ê³„ì‚°

        ì´ìƒ ì ìˆ˜ = -ë³µì› í™•ë¥  (Negative Log Probability)
        í´ìˆ˜ë¡ ì´ìƒì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ

        Args:
            test_data: (n_samples, n_features)

        Returns:
            scores: (n_samples,) ê° ì‹œì ì˜ ì´ìƒ ì ìˆ˜
        """
        if not self.is_fitted:
            raise RuntimeError("Model not fitted. Call fit() first.")

        self.model.eval()

        # ìœˆë„ìš° ìƒì„±
        windows = self._create_windows(test_data)
        windows_tensor = torch.FloatTensor(windows).to(self.device)

        scores_list = []

        with torch.no_grad():
            for i in range(0, len(windows_tensor), self.batch_size):
                batch = windows_tensor[i:i + self.batch_size]

                # ì´ìƒ ì ìˆ˜ (ë§ˆì§€ë§‰ ì‹œì )
                scores = self.model.get_anomaly_score(
                    batch,
                    n_samples=self.n_samples,
                    last_point_only=True
                )
                scores_list.append(scores.cpu().numpy())

        scores = np.concatenate(scores_list, axis=0)

        # ì²« window_length-1 ê°œëŠ” íŒ¨ë”© (ì²« ë²ˆì§¸ ì ìˆ˜ë¡œ)
        padding = np.full(self.window_length - 1, scores[0])
        scores = np.concatenate([padding, scores])

        return scores

    def save(self, path: str) -> None:
        """ëª¨ë¸ ì €ì¥"""
        if not self.is_fitted:
            raise RuntimeError("Model not fitted. Nothing to save.")

        torch.save({
            'model_state_dict': self.model.state_dict(),
            'config': self.config,
            'x_dim': self.x_dim
        }, path)

    def load(self, path: str) -> 'OmniAnomaly':
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device)

        self.config = checkpoint['config']
        self.x_dim = checkpoint['x_dim']

        # ëª¨ë¸ ì¬ìƒì„±
        self.model = OmniAnomalyCore(
            x_dim=self.x_dim,
            z_dim=self.z_dim,
            hidden_dim=self.hidden_dim,
            n_flows=self.n_flows,
            use_flow=self.use_flow
        ).to(self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.is_fitted = True

        return self
